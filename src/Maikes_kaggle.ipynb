{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":75176,"databundleVersionId":8252256,"sourceType":"competition"},{"sourceId":8723652,"sourceType":"datasetVersion","datasetId":5235126},{"sourceId":8741388,"sourceType":"datasetVersion","datasetId":5248304},{"sourceId":8741712,"sourceType":"datasetVersion","datasetId":5248510},{"sourceId":8741741,"sourceType":"datasetVersion","datasetId":5248534}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# kaggle installs\n!pip install torcheval\n!pip install pycocotools --quiet\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\nfrom tqdm.autonotebook import tqdm as tqdm\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms, tv_tensors\nfrom torchvision.io import read_image\nfrom torchvision.transforms import v2\n\n# import medmnist\n# from medmnist import ChestMNIST, DermaMNIST, INFO, Evaluator\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import v2\nfrom torch.optim import lr_scheduler, SGD\nfrom tqdm.autonotebook import tqdm\nfrom torcheval.metrics.functional import multiclass_confusion_matrix\nfrom torchinfo import summary\nimport torchvision\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport json\n\nimport torch.nn.functional as F\nfrom torchvision.transforms.v2 import functional as F2\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport torch.optim as optim\n\n\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN, rpn\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport matplotlib.pyplot as plt\n\nimport random, warnings\n\n\n# --------------- Constants ------------------\n\nROOT = \"/kaggle/input/amia-public-challenge-2024/\"\n# call augment data function\npic_folder_path = ROOT + \"train/train/\"\ninf_folder_path = ROOT + \"test/test/\" # has to be folder\ndict_path = '/kaggle/input/supplements/image_dict.json'\nbatch_size = 10\n\nclass_names = {\n    0: \"Aortic enlargement\",\n    1: \"Atelectasis\",\n    2: \"Calcification\",\n    3: \"Cardiomegaly\",\n    4: \"Consolidation\",\n    5: \"ILD\",\n    6: \"Infiltration\",\n    7: \"Lung Opacity\",\n    8: \"Nodule/Mass\",\n    9: \"Other lesion\",\n    10: \"Pleural effusion\",\n    11: \"Pleural thickening\",\n    12: \"Pneumothorax\",\n    13: \"Pulmonary fibrosis\",\n    14: \"No finding\",\n}\n\nlabel_mapping = {\n    \"14\": 0,  # 'No finding' mapped to 0 (background class)\n    \"0\": 1,\n    \"1\": 2,\n    \"2\": 3,\n    \"3\": 4,\n    \"4\": 5,\n    \"5\": 6,\n    \"6\": 7,\n    \"7\": 8,\n    \"8\": 9,\n    \"9\": 10,\n    \"10\": 11,\n    \"11\": 12,\n    \"12\": 13,\n    \"13\": 14,\n}\n\n\n# Detect OS and set num_workers accordingly\nif os.name == \"nt\":  # Windows\n    num_workers = 0\nelse:  # Linux and others\n    num_workers = 2\n\n\n# --------------- Helper functions ------------------\n# Custom collate function to handle varying sizes of bounding boxes\ndef collate_fn(batch):\n    images, targets = zip(*batch)\n    images = torch.stack(images, dim=0)\n    return images, targets\n\n\ndef get_device():\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        warnings.warn(\"CUDA not available. Using CPU instead.\", UserWarning)\n    print(\"Device set to {}.\".format(device))\n    return device\n\n\n# set seeds for reproducibility\ndef set_seeds(seed=123420):\n    random.seed(seed)\n    np.random.seed(seed + 1)\n    torch.random.manual_seed(seed + 2)\n    device = get_device()\n    if device == \"cuda\":\n        torch.cuda.manual_seed(seed + 3)\n        torch.cuda.manual_seed_all(seed + 4)\n        torch.backends.cudnn.deterministic = True\n    print(\"Seeds set to {}.\".format(seed))\n    return\n\n\ndef string_to_tensor(s):\n    return torch.tensor([ord(c) for c in s], dtype=torch.int64)\n\n\ndef tensor_to_string(t):\n    return \"\".join([chr(c) for c in t])\n\n\nclass GrayscaleImageListDataset(Dataset):\n    def __init__(self, img_dir, img_list, transform=None):\n        self.img_dir = img_dir\n        self.img_list = img_list\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_list[idx])\n        image = read_image(img_path)\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n\ndef get_mean_and_std(\n    img_dir, img_list, batch_size=32, print_values=False, leave_pbar=False\n):\n    \"\"\"\n    Compute the mean and std color values of all images (grayscale values) in the specified list.\n\n    Parameters:\n    - img_dir (str): Directory containing the images.\n    - img_list (list): List of image filenames to include in the calculation.\n    - batch_size (int): Batch size for processing images.\n    - print_values (bool): Whether to print the mean and std values.\n    - leave_pbar (bool): Whether to leave the progress bar after completion.\n\n    Returns:\n    - mean (torch.Tensor): Mean grayscale values.\n    - std (torch.Tensor): Standard deviation of grayscale values.\n    \"\"\"\n    device = get_device()\n    transform = transforms.ToTensor()\n    dataset = GrayscaleImageListDataset(img_dir, img_list, transform=transform)\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n    )\n\n    channels_sum = torch.zeros(1).to(device)\n    channels_squared_sum = torch.zeros(1).to(device)\n    num_pixels = 0\n\n    for images in tqdm(\n        dataloader,\n        desc=\"Calculating mean and std of all grayscale values\",\n        leave=leave_pbar,\n        colour=\"CYAN\",\n    ):\n        images = images.to(device)\n        non_black_pixels = images[images != 0].view(-1)\n        num_pixels += non_black_pixels.shape[0]\n\n        channels_sum += torch.sum(non_black_pixels)\n        channels_squared_sum += torch.sum(non_black_pixels**2)\n\n    mean = channels_sum / num_pixels\n    std = (channels_squared_sum / num_pixels - mean**2) ** 0.5\n\n    if print_values:\n        print(\n            \"Mean: \", mean.cpu().detach().numpy(), \", Std: \", std.cpu().detach().numpy()\n        )\n\n    return mean, std\n\n\n# --------------- Data Loader ------------------\nclass XRayImageDataset(Dataset):\n    \"\"\"\n    load image and targets from dict\n    structure of the dict\n        img_id: {\n            \"classes\": [\n                class_id: [\n                rad_id: [[bbox],[bbox]]]\n    \"\"\"\n\n    def __init__(\n        self,\n        dict,\n        img_size,\n        img_dir,\n        mean=None,\n        std=None,\n        transform_norm=None,\n        nms=True,\n        nms_iou_thresh=0.5,\n    ):\n        self.dict = dict\n        self.keys = list(dict.keys())\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.mean = mean\n        self.std = std\n        self.transform_norm = transform_norm\n        self.nms = nms\n        self.nms_iou_thresh = nms_iou_thresh\n\n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, idx):\n        img_id = self.keys[idx]\n        img_path = os.path.join(self.img_dir, img_id) + \".png\"\n\n        image = read_image(img_path)\n\n        box_list = []\n        label_list = []\n        area_list = []\n        iscrowd_list = []\n        for class_id in self.dict[img_id][\"classes\"]:\n            if class_id == \"14\":\n                continue\n            # collect all boxes for the current class\n            box = []\n            for rad in self.dict[img_id][\"classes\"][class_id].items():\n                for box in rad[1]:\n                    # Ensure the box has 4 coordinates\n                    if len(box) == 4 and box[2] > box[0] and box[3] > box[1]:\n                        box_list.append([coord * image.shape[-1] for coord in box])\n                        label_list.append(label_mapping[class_id])\n                        area_list.append((box[2] - box[0]) * (box[3] - box[1]))\n                        iscrowd_list.append(0)\n                    #else:\n                        #print(f\"Invalid Box found at {img_id} with {box}\")\n            if self.nms:\n                # Non-maximum suppression\n                boxes_to_keep = torchvision.ops.nms(\n                    torch.tensor(box_list).float(),\n                    torch.tensor([1.0] * len(box_list)),\n                    iou_threshold=self.nms_iou_thresh,\n                )\n                # now keep only the values of the indices that are in boxes_to_keep\n                box_list = [box_list[i] for i in boxes_to_keep]\n                label_list = [label_list[i] for i in boxes_to_keep]\n                area_list = [area_list[i] for i in boxes_to_keep]\n                iscrowd_list = [iscrowd_list[i] for i in boxes_to_keep]\n\n        if len(box_list) > 0:\n            boxes_tensor = tv_tensors.BoundingBoxes(\n                box_list, format=\"XYXY\", canvas_size=(image.shape[-1], image.shape[-1])\n            )\n        else:\n            empty_boxes = np.array([]).reshape(-1, 4)\n            boxes_tensor = torch.as_tensor(empty_boxes, dtype=torch.int16)\n        labels_tensor = torch.tensor(label_list, dtype=torch.int64)\n        areas_tensor = torch.tensor(area_list, dtype=torch.int32)\n        iscrowd_tensor = torch.tensor(iscrowd_list, dtype=torch.uint8)\n\n        if self.transform_norm:\n            image, boxes_tensor = self.transform_norm(image, boxes_tensor)\n\n        target = {\n            \"boxes\": boxes_tensor,\n            \"labels\": labels_tensor,\n            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n            \"area\": areas_tensor,\n            \"iscrowd\": iscrowd_tensor,\n            \"filename\": string_to_tensor(img_id),\n        }\n\n        return image, target\n\nclass XRayTestSet(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.img_dir = img_dir\n        self.img_list = os.listdir(img_dir)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_list[idx])\n        image = read_image(img_path)\n        if self.transform:\n            image = self.transform(image)\n        return image, string_to_tensor(self.img_list[idx].split(\".\")[0])\n    \ndef load_and_augment_images(\n    pic_folder_path,\n    inf_folder_path,\n    dict_path,\n    batch_size,\n    class_names,\n    img_size=448,\n    use_normalize=False,\n):\n    # split folders into 70% train and 30% test by ids\n    set_seeds()\n    train_percent = 0.8\n    # Use the images in the ONE folder and split them into train and test\n    train_ids = random.sample(\n        os.listdir(pic_folder_path),\n        int(train_percent * len(os.listdir(pic_folder_path))),\n    )\n    test_ids = [id for id in os.listdir(pic_folder_path) if id not in train_ids]\n\n    # normalize on all train images or use precomputed\n    if use_normalize:\n        mean, std = get_mean_and_std(\n            pic_folder_path, train_ids, print_values=True, leave_pbar=True\n        )\n        print(\"Mean: \", mean, \", Std: \", std)\n    else:\n        mean = 0.57062465\n        std = 0.24919559\n\n    # remove file extension\n    train_ids = [id.split(\".\")[0] for id in train_ids]\n    test_ids = [id.split(\".\")[0] for id in test_ids]\n    # print first values and lengths\n    print(f\"Length of train_ids: {len(train_ids)}\")\n    print(f\"Length of test_ids: {len(test_ids)}\")\n\n    # Data augmentation and normalization for training1\n    data_transforms = {\n        \"train\": v2.Compose(\n            [\n                v2.Resize(img_size, antialias=True),\n                v2.RandomRotation(\n                    degrees=(-6, 6)\n                ),  # all images are upright and will always be. No rotation needed? COuld be interesting to try for generalizing\n                v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.0, hue=0.0),\n                v2.ToDtype(torch.float32, scale=False),\n                v2.RandomPerspective(distortion_scale=0.1, p=0.1),\n                v2.RandomEqualize(p=0.4),\n                v2.Normalize(mean=[mean], std=[std], inplace=True),\n            ]\n        ),\n        \"test\": v2.Compose(\n            [\n                v2.Resize(img_size, antialias=True),\n                v2.ToDtype(torch.float32, scale=False),\n                # Equalize all images to have a more uniform distribution of pixel intensities, regardless of they are generally dark or light\n                v2.RandomEqualize(p=1.0),\n                v2.Normalize(mean=[mean], std=[std], inplace=True),\n            ]\n        ),\n    }\n\n    # load image_dict.json\n    with open(dict_path) as f:\n        og_dict = json.load(f)\n\n    # train_dict where keys match train_ids\n    train_dict = {\n        k: og_dict[k] for k in train_ids\n    }  # if \"14\" not in og_dict[k][\"classes\"]}\n    # print(\"Remaining train dict length: \", len(train_dict))\n    test_dict = {\n        k: og_dict[k] for k in test_ids\n    }  # if \"14\" not in og_dict[k][\"classes\"]}\n    # print(\"Remaining test dict length: \", len(test_dict))\n\n    # size for images\n    img_size = img_size\n    train_dataset = XRayImageDataset(\n        train_dict, img_size, pic_folder_path, mean, std, data_transforms[\"train\"]\n    )\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    test_dataset = XRayImageDataset(\n        test_dict, img_size, pic_folder_path, mean, std, data_transforms[\"test\"]\n    )\n\n    test_dataloader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=1,  # no need for batches\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    # image folder\n    inference_dataset = XRayTestSet(\n        inf_folder_path, data_transforms[\"test\"]\n    )\n\n    inference_dataloader = torch.utils.data.DataLoader(\n        inference_dataset,\n        batch_size=1,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    print(\"Loaded the training dataset.\")\n\n    dataloaders = {\"train\": train_dataloader, \"test\": test_dataloader, \"inference\": inference_dataloader}\n\n    num_classes = class_names.items().__len__()\n\n    return dataloaders, class_names, num_classes\n\n\n# --------------- Model ------------------\nanchor_generator = rpn.AnchorGenerator(\n    sizes=((32,), (48,), (64,), (96,), (128,)),\n    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n)\n\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\", trainable_backbone_layers=5)\nmodel.rpn.anchor_generator = anchor_generator\n# Get the number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# Replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(\n    in_features, 15\n)  # 14 classes + background\n# NOTE: do we have to shift all classes so class 0 is 'No finding'? did this in the label_mapping\n# TODO: Anchor boxes, how to set and adjust them?\n\n\ndef plot_img_bbox(img, target, pred, title):\n    # plot the image and bboxes\n    # different colors for target and pred\n    _, ax = plt.subplots(1, 1, figsize=(10, 10))\n    img = img.cpu().permute(1, 2, 0)\n    ax.imshow(img, cmap=\"gray\")\n    for box in target[\"boxes\"]:\n        box = box.cpu().numpy()\n        rect = plt.Rectangle(\n            (box[0], box[1]),\n            box[2] - box[0],\n            box[3] - box[1],\n            linewidth=2,\n            edgecolor=\"g\",\n            facecolor=\"none\",\n        )\n        ax.add_patch(rect)\n    for box in pred[\"boxes\"]:\n        box = box.cpu().numpy()\n        rect = plt.Rectangle(\n            (box[0], box[1]),\n            box[2] - box[0],\n            box[3] - box[1],\n            linewidth=2,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        ax.add_patch(rect)\n    ax.set_title(title)\n    # plt.savefig(f\"{title}.png\")\n    plt.close()\n\n\ndef train_and_evaluate(model, train_dataloader, val_dataloader, num_epochs=10, lr=0.05):\n    device = get_device()\n    model.to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n    # optimizer = torch.optim.Adamax(params, lr=lr, weight_decay=0.0005)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    exp_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95) # let's try this one as well\n\n    # Initialize MeanAveragePrecision metric\n    metric = MeanAveragePrecision(\n        box_format=\"xyxy\",\n        iou_type=\"bbox\",\n        # iou_thresholds=[0.1],#[0.1, 0.4, 0.7],\n    )\n\n    print(\"Starting the training...\")\n    \n    #-------------------\n    scaler = torch.cuda.amp.GradScaler()\n    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n        model.train()\n        train_loss = 0\n        for images, targets in tqdm(\n            train_dataloader, desc=\"Training\", leave=True, colour=\"BLUE\"\n        ):\n            images = [image.to(device) for image in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            found_invalid_box = False\n            # check if the targets(bounding boxes) are smaller than the image size and have the correct format (positive width and height)\n            for target in targets:\n                for box in target[\"boxes\"]:\n                    if box[2] <= box[0] or box[3] <= box[1]:\n                        found_invalid_box = True\n                        print(f\"Invalid Box found in training with {box}\")\n                    if box[2] > images[0].shape[-1] or box[3] > images[0].shape[-2]:\n                        found_invalid_box = True\n                        print(f\"Box outside of image found in training with {box}\")\n                if found_invalid_box:\n                    print(f\"Image: {tensor_to_string(target['filename'])}\")\n                    raise ValueError(\"Invalid box found in training data\")\n\n           \n            # Apply mixed precision training\n            with torch.cuda.amp.autocast():\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n            train_loss += losses.item()\n\n            optimizer.zero_grad()\n            scaler.scale(losses).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        lr_scheduler.step()  # TODO: adjust scheduler\n\n        #print(f\"Invalid boxes: {inv_boxes}\")\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n\n        model.eval()\n        with torch.no_grad():\n            for images, targets in tqdm(\n                val_dataloader, desc=\"Validation\", leave=True, colour=\"GREEN\"\n            ):\n                images = [image.to(device) for image in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                predictions = model(images)\n\n                filtered_predictions = []\n                filtered_targets = []\n\n                # check if image gets predictions and plot\n                for img, target, pred in zip(images, targets, predictions):\n                    if len(pred[\"boxes\"]) > 0:\n                        '''plot_img_bbox(\n                            img,\n                            target,\n                            pred,\n                            f\"Image {tensor_to_string(target['filename'])}\",\n                        )'''\n                        filtered_predictions.append(pred)\n                        filtered_targets.append(target)\n                \n                # Calculate metrics\n                if len(filtered_predictions) > 0:\n                    metric.update(filtered_predictions, filtered_targets)\n                    #print(f\"Filtered Outputs: {filtered_predictions}\")\n                    #print(f\"Filtered Targets: {filtered_targets}\")\n\n        # Calculate and print the mAP\n        map_metric = metric.compute()\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Val mAP: {map_metric['map']:.4f}\")\n        print(map_metric)\n\n        # Reset the metric for the next epoch\n        metric.reset()\n    print(\"Finished Training!\")\n    # Save the trained model\n    torch.save(model.state_dict(), 'model_state_dict.pth')\n    return model\n\n\n# --------------- Main ------------------\n\ndataloaders, class_names, num_classes = load_and_augment_images(\n    pic_folder_path, inf_folder_path, dict_path, batch_size, class_names\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:26:30.140828Z","iopub.execute_input":"2024-08-07T15:26:30.141741Z","iopub.status.idle":"2024-08-07T15:27:10.429058Z","shell.execute_reply.started":"2024-08-07T15:26:30.141702Z","shell.execute_reply":"2024-08-07T15:27:10.428101Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.9.0)\nDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2690508162.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm as tqdm\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n100%|██████████| 167M/167M [00:01<00:00, 153MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Device set to cuda.\nSeeds set to 123420.\nLength of train_ids: 6858\nLength of test_ids: 1715\nLoaded the training dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = train_and_evaluate(model, dataloaders[\"train\"], dataloaders[\"test\"], 4)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:28:26.902596Z","iopub.execute_input":"2024-08-07T15:28:26.902942Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Device set to cuda.\nStarting the training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70ec764e70943c2985328a4f33c99b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/686 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069156645227437eb049ae069a31f36f"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/4], Train Loss: nan\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/1715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c18c451ba584820aa731296300dcb71"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/4], Val mAP: -1.0000\n{'map': tensor(-1.), 'map_50': tensor(-1.), 'map_75': tensor(-1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(-1.), 'mar_10': tensor(-1.), 'mar_100': tensor(-1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([], dtype=torch.int32)}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanAveragePrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n  warnings.warn(*args, **kwargs)  # noqa: B028\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/686 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ff5142a6384e26aeb8b58153afb337"}},"metadata":{}},{"name":"stdout","text":"Epoch [2/4], Train Loss: nan\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/1715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152edfe28ab7412ab3df86c3eba39a6c"}},"metadata":{}},{"name":"stdout","text":"Epoch [2/4], Val mAP: -1.0000\n{'map': tensor(-1.), 'map_50': tensor(-1.), 'map_75': tensor(-1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(-1.), 'mar_10': tensor(-1.), 'mar_100': tensor(-1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([], dtype=torch.int32)}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/686 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"accbf4efb5984687a0780dd0153d6cad"}},"metadata":{}},{"name":"stdout","text":"Epoch [3/4], Train Loss: nan\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/1715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa80e7ddb3d4fbca8eb376fd4d76f19"}},"metadata":{}},{"name":"stdout","text":"Epoch [3/4], Val mAP: -1.0000\n{'map': tensor(-1.), 'map_50': tensor(-1.), 'map_75': tensor(-1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(-1.), 'mar_1': tensor(-1.), 'mar_10': tensor(-1.), 'mar_100': tensor(-1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([], dtype=torch.int32)}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/686 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17da1e8e98149828e61bed99149c5c5"}},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_state_dict.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef evaluate_and_create_csv(model, test_dataloader, device):\n    model.eval()\n    \n    # Incorporate unix timestamp into the filename\n    output_csv_path = f'submission_{int(time.time())}.csv'\n    \n    # Open the file in write mode and write the header\n    with open(output_csv_path, 'w') as f:\n        f.write(\"ID,TARGET\\n\")  # header row\n        \n        with torch.no_grad():\n            for images, image_ids in tqdm(test_dataloader):\n                images = [image.to(device) for image in images]\n                outputs = model(images)\n                \n                for image_id, output in zip(image_ids, outputs):\n                    boxes = output['boxes'].numpy()\n                    labels = output['labels'].numpy()\n                    scores = output['scores'].numpy()\n                    \n                    row = [tensor_to_string(image_id)]  # Convert image_id tensor to string\n                    if len(boxes) > 0:\n                        targets = []\n                        class_dict = {}\n                        for box, label, score in zip(boxes, labels, scores):\n                            if label not in class_dict or class_dict[label]['score'] < score:\n                                class_dict[label] = {\n                                    'box': [int(b) for b in box],\n                                    'score': score\n                                }\n                        \n                        for label, data in class_dict.items():\n                            box = data['box']\n                            score = data['score']\n                            # switch class back to AMIA format\n                            targets.append(f\"{int(label)-1} {score:.2f} {int(box[0])} {int(box[1])} {int(box[2])} {int(box[3])}\")\n                        \n                        row.append(\" \".join(targets))\n                    else:\n                        # no boxes -> class 14 'No finding'\n                        row.append(\"14 1 0 0 1 1\")\n                    \n                    # Write the row to the file\n                    f.write(f\"{','.join(row)}\\n\")  # image_id, [class score box] [...]\n'''\nmodel.load_state_dict(\n        torch.load(\"/kaggle/input/models2/model_state_dict.pth\",\n        #map_location=torch.device(\"cuda\"\n        ))\n'''\nevaluate_and_create_csv(model, dataloaders[\"inference\"], get_device())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}