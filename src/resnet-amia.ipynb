{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":75176,"databundleVersionId":8252256,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchinfo import summary\nimport torch.nn as nn\nfrom PIL import Image\n\n# Load a pre-trained ResNet-18 model\nmodel = models.resnet18(weights=True)\n\n# Modify the first convolutional layer to accept 1-channel input\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n# Get the number of features in the last layer\nnum_features = model.fc.in_features\n\n# Modify the last fully connected layer to have the same number of output classes as your dataset\nmodel.fc = nn.Linear(num_features, 15)\n\nsummary(model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T14:53:44.527293Z","iopub.execute_input":"2024-05-23T14:53:44.527641Z","iopub.status.idle":"2024-05-23T14:53:51.287052Z","shell.execute_reply.started":"2024-05-23T14:53:44.527613Z","shell.execute_reply":"2024-05-23T14:53:51.286124Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nResNet                                   --\n├─Conv2d: 1-1                            3,136\n├─BatchNorm2d: 1-2                       128\n├─ReLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Sequential: 1-5                        --\n│    └─BasicBlock: 2-1                   --\n│    │    └─Conv2d: 3-1                  36,864\n│    │    └─BatchNorm2d: 3-2             128\n│    │    └─ReLU: 3-3                    --\n│    │    └─Conv2d: 3-4                  36,864\n│    │    └─BatchNorm2d: 3-5             128\n│    └─BasicBlock: 2-2                   --\n│    │    └─Conv2d: 3-6                  36,864\n│    │    └─BatchNorm2d: 3-7             128\n│    │    └─ReLU: 3-8                    --\n│    │    └─Conv2d: 3-9                  36,864\n│    │    └─BatchNorm2d: 3-10            128\n├─Sequential: 1-6                        --\n│    └─BasicBlock: 2-3                   --\n│    │    └─Conv2d: 3-11                 73,728\n│    │    └─BatchNorm2d: 3-12            256\n│    │    └─ReLU: 3-13                   --\n│    │    └─Conv2d: 3-14                 147,456\n│    │    └─BatchNorm2d: 3-15            256\n│    │    └─Sequential: 3-16             8,448\n│    └─BasicBlock: 2-4                   --\n│    │    └─Conv2d: 3-17                 147,456\n│    │    └─BatchNorm2d: 3-18            256\n│    │    └─ReLU: 3-19                   --\n│    │    └─Conv2d: 3-20                 147,456\n│    │    └─BatchNorm2d: 3-21            256\n├─Sequential: 1-7                        --\n│    └─BasicBlock: 2-5                   --\n│    │    └─Conv2d: 3-22                 294,912\n│    │    └─BatchNorm2d: 3-23            512\n│    │    └─ReLU: 3-24                   --\n│    │    └─Conv2d: 3-25                 589,824\n│    │    └─BatchNorm2d: 3-26            512\n│    │    └─Sequential: 3-27             33,280\n│    └─BasicBlock: 2-6                   --\n│    │    └─Conv2d: 3-28                 589,824\n│    │    └─BatchNorm2d: 3-29            512\n│    │    └─ReLU: 3-30                   --\n│    │    └─Conv2d: 3-31                 589,824\n│    │    └─BatchNorm2d: 3-32            512\n├─Sequential: 1-8                        --\n│    └─BasicBlock: 2-7                   --\n│    │    └─Conv2d: 3-33                 1,179,648\n│    │    └─BatchNorm2d: 3-34            1,024\n│    │    └─ReLU: 3-35                   --\n│    │    └─Conv2d: 3-36                 2,359,296\n│    │    └─BatchNorm2d: 3-37            1,024\n│    │    └─Sequential: 3-38             132,096\n│    └─BasicBlock: 2-8                   --\n│    │    └─Conv2d: 3-39                 2,359,296\n│    │    └─BatchNorm2d: 3-40            1,024\n│    │    └─ReLU: 3-41                   --\n│    │    └─Conv2d: 3-42                 2,359,296\n│    │    └─BatchNorm2d: 3-43            1,024\n├─AdaptiveAvgPool2d: 1-9                 --\n├─Linear: 1-10                           7,695\n=================================================================\nTotal params: 11,177,935\nTrainable params: 11,177,935\nNon-trainable params: 0\n================================================================="},"metadata":{}}]},{"cell_type":"code","source":"ROOT = \"/kaggle/input/amia-public-challenge-2024\"\n\ntest_img_path = ROOT + \"/test/test\"\ntrain_img_path = ROOT + \"/train/train\"\n\ntest_annot_path = ROOT + \"/test.csv\"\ntrain_annot_path = ROOT + \"/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:53:51.289169Z","iopub.execute_input":"2024-05-23T14:53:51.289950Z","iopub.status.idle":"2024-05-23T14:53:51.294258Z","shell.execute_reply.started":"2024-05-23T14:53:51.289912Z","shell.execute_reply":"2024-05-23T14:53:51.293290Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass XRayImageDataset(Dataset):\n    \n    def __init__(self, annotations_file, img_dir, mean=None, std=None,transform_norm=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.mean = mean\n        self.std = std\n        self.transform_norm = transforms.Compose([\n            transforms.Normalize(self.mean, self.std)\n        ])\n        self.target_transform = target_transform    \n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])+'.png'\n        image = read_image(img_path) # PyTorch function, no need to change\n        label = self.img_labels.iloc[idx, 2] # class_id column\n        image = self.transform_norm(image.float())\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:53:51.295468Z","iopub.execute_input":"2024-05-23T14:53:51.295798Z","iopub.status.idle":"2024-05-23T14:53:51.605183Z","shell.execute_reply.started":"2024-05-23T14:53:51.295768Z","shell.execute_reply":"2024-05-23T14:53:51.604392Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\nfrom torchvision.io import read_image\n\ntrain_data = XRayImageDataset(train_annot_path, train_img_path, 0.04664242120787185, 0.10213025072799406)\nprint(f'Training images: mean {train_data.mean}, std {train_data.std}')\n\ntrain_dataloader = DataLoader(train_data, batch_size=12, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:53:51.606947Z","iopub.execute_input":"2024-05-23T14:53:51.607406Z","iopub.status.idle":"2024-05-23T14:53:51.729601Z","shell.execute_reply.started":"2024-05-23T14:53:51.607379Z","shell.execute_reply":"2024-05-23T14:53:51.728569Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training images: mean 0.04664242120787185, std 0.10213025072799406\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = XRayImageDataset(test_annot_path, test_img_path, 0.07274098403775907, 0.16118353533641264)\nprint(f'Testing images: mean {test_data.mean}, std {test_data.std}')\n\ntest_dataloader = DataLoader(test_data, batch_size=12, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:53:51.730690Z","iopub.execute_input":"2024-05-23T14:53:51.730981Z","iopub.status.idle":"2024-05-23T14:53:51.759479Z","shell.execute_reply.started":"2024-05-23T14:53:51.730939Z","shell.execute_reply":"2024-05-23T14:53:51.758621Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Testing images: mean 0.07274098403775907, std 0.16118353533641264\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Number of epochs\nnum_epochs = 2\n\n# Device configuration (use GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Training and evaluation loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(train_dataloader, desc=f'Train Ep. 1 {epoch+1}'):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n\n    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')\n\n    # Evaluation phase\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_dataloader, desc=f'Evaluating Epoch {epoch+1}/{num_epochs}'):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Statistics\n            test_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_loss /= total\n    test_acc = correct / total\n\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n\nprint('Training complete')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:53:51.760762Z","iopub.execute_input":"2024-05-23T14:53:51.761036Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Train Ep. 1 1:  16%|█▌        | 611/3828 [06:17<33:09,  1.62it/s]","output_type":"stream"}]}]}