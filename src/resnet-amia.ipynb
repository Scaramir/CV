{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":75176,"databundleVersionId":8252256,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchinfo import summary\nimport torch.nn as nn\nfrom PIL import Image\n\n# Load a pre-trained ResNet-18 model\nmodel = models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\n\n# Freeze all the parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Modify the first convolutional layer to accept 1-channel input\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n# Unfreeze the parameters of the first convolutional layer\nfor param in model.conv1.parameters():\n    param.requires_grad = True\n\n# Get the number of features in the last layer\nnum_features = model.fc.in_features\n\n# Modify the last fully connected layer to have the same number of output classes as your dataset\nmodel.fc = nn.Linear(num_features, 15)\n\n# Ensure the parameters of the fully connected layer are trainable\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\nsummary(model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = \"/kaggle/input/amia-public-challenge-2024\"\n\ntest_img_path = ROOT + \"/test/test\"\ntrain_img_path = ROOT + \"/train/train\"\n\ntest_annot_path = ROOT + \"/test.csv\"\ntrain_annot_path = ROOT + \"/train.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the train CSV file\ntrain_df = pd.read_csv(train_annot_path)\ntest_df = pd.read_csv(test_annot_path)\nprint(train_df.head())\nprint(train_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by image_id to ensure each image stays in one set\nunique_image_ids = train_df['image_id'].unique()\nprint(f\"Unique Train IDs: {len(unique_image_ids)}\")\n\nfrom torch.utils.data import random_split\n# Split into train and validation sets\ntrain_split = 0.7\ntrain_size = int(train_split * len(unique_image_ids))\nval_size = len(unique_image_ids) - train_size\n\n# Ensure reproducibility\ntorch.manual_seed(42)\n\n# Get the indices of the train and validation sets\ntrain_indices, val_indices = random_split(unique_image_ids.tolist(), [train_size, val_size])\nprint(f\"Train indices: {len(train_indices)}\")\nprint(f\"Val indices: {len(val_indices)}\")\n\n# Get the corresponding image IDs\ntrain_ids = [unique_image_ids[i] for i in train_indices.indices]\nval_ids = [unique_image_ids[i] for i in val_indices.indices]\n\n# Create train and validation dataframes\ntrain_data_df = train_df[train_df['image_id'].isin(train_ids)]\nval_data_df = train_df[train_df['image_id'].isin(val_ids)]\n\n# Check the number of unique images in each set\ntrain_unique_images = train_data_df['image_id'].nunique()\nval_unique_images = val_data_df['image_id'].nunique()\n\nprint(f\"Training samples: {train_data_df.shape[0]}, Training unique images: {train_unique_images}\")\nprint(f\"Validation samples: {val_data_df.shape[0]}, Validation unique images: {val_unique_images}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass XRayImageDataset(Dataset):\n    \n    def __init__(self, dataframe, img_dir, mean=None, std=None,transform_norm=None, target_transform=None):\n        self.img_labels = dataframe\n        self.img_dir = img_dir\n        self.mean = mean\n        self.std = std\n        self.transform_norm = transforms.Compose([\n            transforms.Resize((224,224),antialias=True),\n            transforms.Normalize(self.mean, self.std)\n        ])\n        self.target_transform = target_transform    \n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_id = self.img_labels.iloc[idx]['image_id']\n        img_path = os.path.join(self.img_dir, img_id) + '.png'\n        image = read_image(img_path).float() # PyTorch function, no need to change\n        label = self.img_labels.iloc[idx]['class_id'] # class_id column\n        image = self.transform_norm(image)\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\nfrom torchvision.io import read_image\n\ntrain_mean = 0.04664242120787185\ntrain_std = 0.10213025072799406\n# TODO: improvement for separate train / val computations?\n\ntrain_batch_size = 64\n\ntrain_dataset = XRayImageDataset(train_data_df, train_img_path, train_mean, train_std)\nprint(f'Training images: mean {train_dataset.mean}, std {train_dataset.std}')\ntrain_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\nval_dataset = XRayImageDataset(val_data_df, train_img_path, train_mean, train_std)\nval_dataloader = DataLoader(val_dataset, batch_size=train_batch_size*2, shuffle=True)\nprint(f'Using same metrics for validation at {train_split}/{1-train_split} ratio')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_mean = 0.07274098403775907\ntest_std = 0.16118353533641264\n\ntest_batch_size = 32\n\ntest_dataset = XRayImageDataset(test_df, test_img_path, test_mean, test_std)\nprint(f'Testing images: mean {test_dataset.mean}, std {test_dataset.std}')\ntest_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Number of epochs\nnum_epochs = 10\n\n# Device configuration (use GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Training and evaluation loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(train_dataloader, desc=f'Train Ep. {epoch+1}'):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n\n    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_dataloader, desc=f'Validating Ep. {epoch+1}/{num_epochs}'):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Statistics\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= total\n    val_acc = correct / total\n\n    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n\nprint('Training complete')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}