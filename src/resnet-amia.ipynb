{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torchvision.tv_tensors as tv\n","from torchinfo import summary\n","import torch.nn as nn\n","from PIL import Image\n","\n","# Load a pre-trained ResNet-18 model\n","model = models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\n","\n","# Freeze all the parameters\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Modify the first convolutional layer to accept 1-channel input\n","model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","# Unfreeze the parameters of the first convolutional layer\n","for param in model.conv1.parameters():\n","    param.requires_grad = True\n","\n","# Get the number of features in the last layer\n","num_features = model.fc.in_features\n","\n","# Modify the last fully connected layer to have the same number of output classes as your dataset\n","model.fc = nn.Linear(num_features, 15)\n","\n","# Ensure the parameters of the fully connected layer are trainable\n","for param in model.fc.parameters():\n","    param.requires_grad = True\n","\n","summary(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ROOT = \"/kaggle/input/amia-public-challenge-2024\"\n","\n","test_img_path = ROOT + \"/test/test\"\n","train_img_path = ROOT + \"/train/train\"\n","\n","test_annot_path = ROOT + \"/test.csv\"\n","train_annot_path = ROOT + \"/train.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load the train CSV file\n","train_df = pd.read_csv(train_annot_path)\n","test_df = pd.read_csv(test_annot_path)\n","print(train_df.head())\n","print(train_df.columns)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('bM8C97htulC9fHKIDurJHquCXr1KZuug', {'classes': {'14.0': {'R5': [[0.0, 0.0, 0.0005015045135406219, 0.00041152263374485596]], 'R3': [[0.0, 0.0, 0.0005015045135406219, 0.00041152263374485596]], 'R17': [[0.0, 0.0, 0.0005015045135406219, 0.00041152263374485596]]}}, 'og_dims': [2430, 1994]})\n"]}],"source":["# Load the Json file\n","import json\n","\n","dict_path = 'pre-pro/image_dict.json'\n","\n","with open(dict_path) as f:\n","    train_dict = json.load(f)\n","\n","# print first enry of the dictionary\n","print(list(train_dict.items())[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Group by image_id to ensure each image stays in one set\n","unique_image_ids = train_df['image_id'].unique()\n","print(f\"Unique Train IDs: {len(unique_image_ids)}\")\n","\n","from torch.utils.data import random_split\n","# Split into train and validation sets\n","train_split = 0.7\n","train_size = int(train_split * len(unique_image_ids))\n","val_size = len(unique_image_ids) - train_size\n","\n","# Ensure reproducibility\n","torch.manual_seed(42)\n","\n","# Get the indices of the train and validation sets\n","train_indices, val_indices = random_split(unique_image_ids.tolist(), [train_size, val_size])\n","print(f\"Train indices: {len(train_indices)}\")\n","print(f\"Val indices: {len(val_indices)}\")\n","\n","# Get the corresponding image IDs\n","train_ids = [unique_image_ids[i] for i in train_indices.indices]\n","val_ids = [unique_image_ids[i] for i in val_indices.indices]\n","\n","# Create train and validation dataframes\n","train_data_df = train_df[train_df['image_id'].isin(train_ids)]\n","val_data_df = train_df[train_df['image_id'].isin(val_ids)]\n","\n","# Check the number of unique images in each set\n","train_unique_images = train_data_df['image_id'].nunique()\n","val_unique_images = val_data_df['image_id'].nunique()\n","\n","print(f\"Training samples: {train_data_df.shape[0]}, Training unique images: {train_unique_images}\")\n","print(f\"Validation samples: {val_data_df.shape[0]}, Validation unique images: {val_unique_images}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","\n","class XRayImageDataset(Dataset):\n","    \n","    def __init__(self, dict , img_dir, size, mean=None, std=None,transform_norm=None, target_transform=None):\n","        self.dict = dict\n","        self.img_dir = img_dir\n","        self.keys = list(dict.keys())\n","        self.size = size\n","        self.mean = mean\n","        self.std = std\n","        self.transform_norm = transforms.Compose([\n","            transforms.Resize((224,224),antialias=True),\n","            transforms.Normalize(self.mean, self.std)\n","        ])\n","        self.target_transform = target_transform    \n","\n","    def __len__(self):\n","        return len(self.keys)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.keys[idx]\n","        img_path = os.path.join(self.img_dir, img_id) + '.png'\n","        image = read_image(img_path).float() # PyTorch function, no need to change\n","        image = self.transform_norm(image)\n","        # https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","        \n","        \n","        box_list = []\n","        label_list = []\n","        area_list = []\n","        iscrowd_list = []\n","        # get all boxes of all radiologists\n","        # structure of the dict\n","        # img_id: {\n","        #     \"classes\": [\n","        #         class_id: [\n","        #            rad_id: [[bbox],[bbox]]]\n","        \n","        for class_id in self.dict[img_id][\"classes\"].items():\n","            for rad in class_id.items():\n","                for box in rad.items():\n","                    box = box * self.size\n","                    box_list.append(box)\n","                    label_list.append(class_id)\n","                    area_list.append((box[2]-box[0])*(box[3]-box[1])) # x_max-x_min * y_max-y_min\n","                    iscrowd_list.append(0)\n","        \n","        # target dict\n","        target = {\n","            \"boxes\": tv.BoundingBoxes(box_list),\n","            \"labels\": label_list,\n","            \"image_id\": torch.tensor([idx]),\n","            \"area\": torch.tensor(area_list),\n","            \"iscrowd\": torch.tensor(iscrowd_list)\n","        }\n","\n","        return image, target, img_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets, transforms\n","from torchvision.io import read_image\n","\n","train_mean = 0.04664242120787185\n","train_std = 0.10213025072799406\n","# TODO: improvement for separate train / val computations?\n","\n","train_batch_size = 64\n","\n","train_dataset = XRayImageDataset(train_data_df, train_img_path, train_mean, train_std)\n","print(f'Training images: mean {train_dataset.mean}, std {train_dataset.std}')\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","\n","val_dataset = XRayImageDataset(val_data_df, train_img_path, train_mean, train_std)\n","val_dataloader = DataLoader(val_dataset, batch_size=train_batch_size*2, shuffle=True)\n","print(f'Using same metrics for validation at {train_split}/{1-train_split} ratio')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# TODO: use mean and std from training set?\n","test_mean = 0.07274098403775907\n","test_std = 0.16118353533641264\n","\n","test_batch_size = 32\n","\n","test_dataset = XRayImageDataset(test_df, test_img_path, test_mean, test_std)\n","print(f'Testing images: mean {test_dataset.mean}, std {test_dataset.std}')\n","test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Number of epochs\n","num_epochs = 10\n","\n","# Device configuration (use GPU if available)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Training and evaluation loop\n","for epoch in range(num_epochs):\n","    # Training phase\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in tqdm(train_dataloader, desc=f'Train Ep. {epoch+1}'):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(val_dataloader, desc=f'Validating Ep. {epoch+1}/{num_epochs}'):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Statistics\n","            val_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    val_loss /= total\n","    val_acc = correct / total\n","\n","    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n","\n","print('Training complete')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8252256,"sourceId":75176,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
